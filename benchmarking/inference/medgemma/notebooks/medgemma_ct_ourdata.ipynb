{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MedGemma 1.5 on Our CT Dataset (NIfTI)\n",
        "\n",
        "This notebook mirrors the official `high_dimensional_ct_hugging_face.ipynb`, but loads CT volumes from our dataset (NIfTI) and samples slices for MedGemma input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "Install required dependencies (if not already installed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If needed\n",
        "# !pip install nibabel pydicom transformers pillow tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure paths and settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Path to our dataset JSONL (QA)\n",
        "DATASET_JSONL = Path(\"3D_VLM_Spatial/spatial_qa_processed.jsonl\")\n",
        "\n",
        "# Root of CT NIfTI volumes (valid_fixed)\n",
        "NIFTI_ROOT = Path(\"3D_VLM_Spatial/dataset/data_volumes/dataset/valid_fixed\")\n",
        "\n",
        "# MedGemma model ID\n",
        "MODEL_ID = \"google/medgemma-1.5-4b-it\"\n",
        "\n",
        "# Number of slices to sample uniformly\n",
        "NUM_SLICES = 85\n",
        "\n",
        "# Optional resize\n",
        "RESIZE_LONGEST = 384  # set 0 to disable\n",
        "PAD_SQUARE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers: load volume, sample slices, windowing\n",
        "(Directly adapted from the official notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "import io, base64\n",
        "\n",
        "def derive_nifti_path(nifti_root: Path, case_id: str) -> Path:\n",
        "    stem = case_id.replace(\".nii.gz\", \"\")\n",
        "    subdir = stem.rsplit(\"_\", 1)[0]  # valid_1_a\n",
        "    base = subdir.rsplit(\"_\", 1)[0] if \"_\" in subdir else subdir\n",
        "    return nifti_root / base / subdir / case_id\n",
        "\n",
        "def sample_indices(n_slices: int, num_samples: int):\n",
        "    if n_slices <= num_samples:\n",
        "        return list(range(n_slices))\n",
        "    idxs = np.linspace(0, n_slices - 1, num_samples)\n",
        "    return [int(round(i)) for i in idxs]\n",
        "\n",
        "# Windowing as in the official notebook\n",
        "WINDOW_CLIPS = [(-1024, 1024), (-135, 215), (0, 80)]\n",
        "\n",
        "def norm(ct_vol: np.ndarray, lo: float, hi: float) -> np.ndarray:\n",
        "    ct_vol = np.clip(ct_vol, lo, hi).astype(np.float32)\n",
        "    ct_vol -= lo\n",
        "    ct_vol /= (hi - lo)\n",
        "    ct_vol *= 255.0\n",
        "    return ct_vol\n",
        "\n",
        "def window(ct_slice: np.ndarray) -> np.ndarray:\n",
        "    return np.stack([norm(ct_slice, lo, hi) for (lo, hi) in WINDOW_CLIPS], axis=-1)\n",
        "\n",
        "def resize_rgb(rgb: np.ndarray, longest: int, pad_square: bool):\n",
        "    if longest <= 0:\n",
        "        return rgb\n",
        "    h, w = rgb.shape[:2]\n",
        "    scale = longest / float(max(h, w))\n",
        "    new_w = max(1, int(round(w * scale)))\n",
        "    new_h = max(1, int(round(h * scale)))\n",
        "    img = Image.fromarray(rgb)\n",
        "    img = img.resize((new_w, new_h), resample=Image.BILINEAR)\n",
        "    if not pad_square:\n",
        "        return np.asarray(img)\n",
        "    canvas = Image.new(\"RGB\", (longest, longest), (0, 0, 0))\n",
        "    x0 = (longest - new_w) // 2\n",
        "    y0 = (longest - new_h) // 2\n",
        "    canvas.paste(img, (x0, y0))\n",
        "    return np.asarray(canvas)\n",
        "\n",
        "# Load one volume (example)\n",
        "with DATASET_JSONL.open() as f:\n",
        "    example = json.loads(next(f))\n",
        "\n",
        "case_id = example[\"case_id\"]\n",
        "question = example[\"question\"]\n",
        "\n",
        "nifti_path = derive_nifti_path(NIFTI_ROOT, case_id)\n",
        "print(\"Example NIfTI:\", nifti_path)\n",
        "\n",
        "vol = np.asarray(nib.load(str(nifti_path)).get_fdata())\n",
        "# Nibabel loads as (x, y, z) -> transpose to (z, y, x)\n",
        "vol = np.transpose(vol, (2,1,0))\n",
        "\n",
        "idxs = sample_indices(vol.shape[0], NUM_SLICES)\n",
        "\n",
        "slices = []\n",
        "for i in idxs:\n",
        "    rgb = window(vol[i])\n",
        "    rgb = np.round(rgb, 0).astype(np.uint8)\n",
        "    rgb = resize_rgb(rgb, RESIZE_LONGEST, PAD_SQUARE)\n",
        "    slices.append(rgb)\n",
        "\n",
        "print(\"Slices:\", len(slices), slices[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build MedGemma prompt (chat template)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
        "\n",
        "instruction = (\n",
        "    \"You are an instructor teaching medical students. You are \"\n",
        "    \"analyzing a contiguous block of CT slices. Please review the slices provided below carefully.\"\n",
        ")\n",
        "query_suffix = (\n",
        "    \"\n",
        "\n",
        "Based on the visual evidence in the slices provided above, \"\n",
        "    \"answer the question below. Provide concise reasoning and conclude with a final answer.\"\n",
        ")\n",
        "\n",
        "def _encode(rgb: np.ndarray, fmt: str = \"jpeg\") -> str:\n",
        "    with io.BytesIO() as buf:\n",
        "        Image.fromarray(rgb).save(buf, format=fmt)\n",
        "        buf.seek(0)\n",
        "        encoded = base64.b64encode(buf.getbuffer()).decode(\"utf-8\")\n",
        "    return f\"data:image/{fmt};base64,{encoded}\"\n",
        "\n",
        "content = [{\"type\": \"text\", \"text\": instruction}]\n",
        "for i, rgb in enumerate(slices, 1):\n",
        "    content.append({\"type\": \"image\", \"image\": _encode(rgb)})\n",
        "    content.append({\"type\": \"text\", \"text\": f\"SLICE {i}\"})\n",
        "content.append({\"type\": \"text\", \"text\": f\"{query_suffix}\n",
        "\n",
        "Question: {question}\"})\n",
        "\n",
        "messages = [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    continue_final_message=False,\n",
        "    return_tensors=\"pt\",\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        ")\n",
        "\n",
        "print(\"Input tokens:\", inputs[\"input_ids\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load MedGemma and run inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageTextToText\n",
        "\n",
        "model_kwargs = dict(\n",
        "    dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    offload_buffers=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(MODEL_ID, **model_kwargs)\n",
        "\n",
        "# Move tensors to device\n",
        "inputs = {k: (v.to(model.device, dtype=torch.bfloat16) if torch.is_floating_point(v) else v.to(model.device))\n",
        "          for k, v in inputs.items()}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    generated = model.generate(**inputs, do_sample=False, max_new_tokens=512)\n",
        "\n",
        "raw_output = processor.post_process_image_text_to_text(generated, skip_special_tokens=True)[0]\n",
        "input_text = processor.post_process_image_text_to_text(inputs[\"input_ids\"], skip_special_tokens=True)[0]\n",
        "\n",
        "# Remove the input prompt from the output if present\n",
        "out = raw_output\n",
        "idx = out.find(input_text)\n",
        "if 0 <= idx <= 2:\n",
        "    out = out[idx + len(input_text):].strip()\n",
        "\n",
        "print(out)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}