# Preprocessing + Input Summary (by model)

| Model | Input fed to model | Preprocessing / transforms (actual code) | View / slices used | Code reference |
|---|---|---|---|---|
| **Med3DVLM** | Full 3D volume tensor `(1,1,D,H,W)` (from NIfTI or `.npy`) | `SimpleITK` read; if 3D, expand dims; **no HU clip, no windowing, no resize** in `run_custom_eval.py`. | Uses **full volume** (not single slice). | `benchmarking/inference/med3dvlm/run_custom_eval.py` (`prepare_image`, `load_volume`) |
| **Merlin** | Full 3D volume tensor `[1,1,D,H,W]` | **Reorients to RAS**, resamples to spacing `(1.5,1.5,3.0)`; clip HU to `[-1000,1000]`; normalize to `[0,1]`; **center pad/crop** to `(160,224,224)` | Full volume | `benchmarking/inference/merlin/run_custom_eval.py` (`load_volume`, `center_pad_crop`) |
| **VILA‑M3** | **Single 2D axial slice** converted to PIL, then processed by LLaVA/BLIP image processor | Loads **`.npy` volume** expected `(D,H,W)` or `(1,D,H,W)`; takes **middle axial slice**; maps from `[-1,1] → [0,255]`; no HU windowing. | **Single slice** (default mid axial) | `benchmarking/inference/vila-m3/run_custom_eval.py` (`volume_to_pil`) |
| **M3D‑LaMed** | Full 3D volume tensor `(1,1,D,H,W)` | Loads `.npy` expected `(D,H,W)` or `(1,D,H,W)`; **no HU windowing, no resize** in eval script. | Full volume | `benchmarking/inference/m3d/run_custom_eval.py` (`load_image`) |
| **CT‑CLIP (encoder used by CT‑CHAT)** | **Encoded visual tokens** from CTViT | For CT‑CHAT encoding: resample to spacing `(0.75,0.75,1.5)` (z,xy); HU clip `[-1000,1000]`; normalize by `/1000` ⇒ `[-1,1]`; transpose to `(H,W,D)`; **center crop/pad** to `(480,480,240)`; then encode with CTViT. | Full 3D volume → encoded tokens | `benchmarking/inference/ct-chat/encode_embeddings.py` (`nii_to_tensor`) |
| **RadFM** | Full 3D tensor (from `.npz`/`.npy`) | Loads `npz/npy`; expects **`(3,H,W,D)` or `(H,W,D)`**; **no HU windowing or resize** in eval script. | Full volume | `benchmarking/inference/radfm/run_custom_eval.py` (`load_image`) |
| **MedEvalKit (Lingshu‑7B)** | **N slices** (default 32) converted to RGB windows | NIfTI loaded with `nibabel`; transpose to **`(Z,Y,X)`**; **3 window RGB**: `[-1024,1024]`, `[-135,215]`, `[0,80]`; optional resize; prompt includes **all slices**. | **Uniform N slices** across volume | `benchmarking/inference/medevalkit/run_custom_eval.py` (`load_volume`, `make_rgb_window`, `sample_indices`) |
| **MedGemma** | **N slices** (default 85) encoded inline as base64 images | NIfTI loaded via `nibabel`; transpose to **`(Z,Y,X)`**; **3 window RGB**: `[-1024,1024]`, `[-135,215]`, `[0,80]`; optional resize/resize‑longest; **adaptive downsample** if prompt exceeds context; inline base64 images in prompt. | **Uniform N slices** across volume (auto‑reduced if too long) | `benchmarking/inference/medgemma/run_custom_eval.py` (`load_volume`, `make_rgb_window`, `_downsample_slices`) |

## Notes
- **VILA‑M3 and M3D** expect `.npy` volumes in preprocessed ranges. If those `.npy` files were created elsewhere, their preprocessing **is not in these eval scripts**.
- The **view mismatches** in your grid are caused by inconsistent storage order of volumes (some are `D,H,W`, others `H,W,D`). This must be handled per‑model when generating PNGs.
- **CT‑CLIP** itself is not directly used for QA; in our pipeline it’s used to create CT‑CHAT embeddings (`.npz` tokens). If you want CT‑CLIP visualization, you need to decide which volume representation to visualize (raw vs encoded vs preprocessed).
