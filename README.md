# Benchmarking 3D Medical Vision-Language Models for Spatial Understanding

> **A comprehensive evaluation framework for assessing spatial reasoning capabilities in 3D medical vision-language models using volumetric CT imaging.**

![Paper Status](https://img.shields.io/badge/Status-MICCAI%202026-blue)
![Dataset](https://img.shields.io/badge/Dataset-CT--RATE-green)
![Models](https://img.shields.io/badge/Models-8%2B-orange)

## üìã Overview

This repository contains the evaluation framework, dataset, and benchmarking code for assessing how well 3D medical vision-language models (VLMs) understand spatial relationships in volumetric CT imaging. We introduce a curated **spatial QA benchmark** that evaluates models on their ability to reason about anatomical positions, distances, laterality, and inter-organ relationships‚Äîcritical capabilities for clinical deployment.

### Key Contributions

- **Spatial QA Benchmark**: A curated dataset of ~1,000 spatial reasoning questions derived from CT radiology reports with LLM-validated ground truth
- **Multi-Model Evaluation**: Comprehensive evaluation of 8+ state-of-the-art 3D medical VLMs including Med3DVLM, CT-Chat, M3D, RadFM, MedGemma, Merlin, VILA-M3, and others
- **Spatial Classification Framework**: Automated spatial vs. non-spatial question filtering using LLM judges (Gemini 2.0)
- **Flexible Benchmarking Pipeline**: Modular evaluation infrastructure supporting multiple models, input formats, and evaluation metrics
- **Reproducibility**: Complete preprocessing, inference, and evaluation scripts with detailed documentation

## üéØ Research Questions

This work addresses:

1. **Do 3D medical VLMs understand spatial relationships?** How well do they localize findings and reason about anatomical positions?
2. **What types of spatial reasoning are challenging?** Laterality, distance comparison, volumetric assessment, or complex multi-organ reasoning?
3. **How do different model architectures compare** on spatial understanding despite similar overall medical VLM capabilities?
4. **What role does input representation play?** How do different 3D slicing strategies (axial, coronal, sagittal) affect spatial reasoning?

## üìä Dataset

### CT-RATE Source

We utilize the **CT-RATE dataset** (CT Radiology Assessment Training Exam), a comprehensive collection of CT scans with expert radiologist reports. The dataset provides:

- **~130 CT volumes** from various anatomical regions and pathologies
- **Structured radiology reports** with findings and impressions sections
- **Diverse pathologies** including tumors, infections, structural abnormalities
- **English language reports** with detailed anatomical descriptions

### Spatial QA Pairs

From each case's findings and impressions, we generate spatial question-answer pairs targeting:

- **Laterality reasoning**: "Which lung contains the lesion?"
- **Distance & proximity**: "What is the spatial relationship between the nodule and the pleura?"
- **Volumetric assessment**: "Which lesion is larger in volume?"
- **Anatomical localization**: "In which segment of the liver is the lesion located?"
- **Multi-organ comparisons**: "Is the right kidney larger than the left?"

### Quality Assurance

All QA pairs are validated using **Google Gemini 2.0 Flash** with custom prompts to ensure:

- **Spatial relevance**: Questions genuinely require spatial reasoning about anatomical location, orientation, or relative position
- **Ground truth validity**: Answers are inferable from the imaging findings and radiologist annotations
- **Non-redundancy**: Removal of textual paraphrasing or general knowledge questions

**Final dataset**: ~1,000 high-quality spatial QA pairs with validated ground truth

## üèóÔ∏è Repository Structure

```
MICCAI2026-3DMedVLMS/
‚îú‚îÄ‚îÄ 3D_VLM_Spatial/                    # Core benchmarking framework
‚îÇ   ‚îú‚îÄ‚îÄ dataset/                       # CT-RATE download & validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ download_dataset.py       # Script to fetch CT volumes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ preprocess/                    # Model-specific preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_m3d.py         # M3D: 256√ó256√ó32 NIfTI ‚Üí .npy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_med3dvlm.py    # Med3DVLM format
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_merlin.py      # Merlin preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_radfm.py       # RadFM preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_ctchat.py      # CT-Chat preprocessing
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...                       # Additional model preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ qa_generation_v2/             # QA pair generation pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qa_generation.py          # Generate QA from reports
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prepare_case_split.py     # Manage case sampling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ spatial_qa_*.jsonl        # Generated QA datasets
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ reports/                      # Generated reports & judgments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_judgments.json     # LLM spatial classification
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ *_predictions_*.jsonl    # Model outputs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ *_eval.json              # Evaluation metrics
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                      # Analysis & evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ filter_qa_pairs.py       # Remove non-spatial QAs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ to_jsonl.py              # Convert JSON ‚Üí JSONL
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_predictions.py  # Exact-match accuracy
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_with_gemini.py  # LLM-based evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_text_metrics.py # BLEU, ROUGE, METEOR
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluate_with_alignscore.py # Factual consistency
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ build_correctness_matrix.py # Cross-model comparison
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ plot_answer_length_distributions.py # Analysis
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ README.md                    # Detailed 3D_VLM_Spatial guide
‚îÇ   ‚îú‚îÄ‚îÄ spatial_qa_output.json       # Master QA list
‚îÇ   ‚îú‚îÄ‚îÄ spatial_qa_filtered.json     # After spatial filtering
‚îÇ   ‚îî‚îÄ‚îÄ validation_reports_output.json  # Radiology reports
‚îÇ
‚îú‚îÄ‚îÄ benchmarking/                     # Model inference harness
‚îÇ   ‚îú‚îÄ‚îÄ inference/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ med3dvlm/                 # Med3DVLM eval
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_custom_eval.py   # Inference script
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ m3d/                      # M3D eval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ merlin/                   # Merlin eval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ radfm/                    # RadFM eval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ctchat/                   # CT-Chat eval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ medgemma/                 # MedGemma eval
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vila-m3/                  # VILA-M3 + VISTA3D eval
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [other-models]/
‚îÇ   ‚îî‚îÄ‚îÄ README.md                     # Benchmarking pipeline overview
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                      # Model weights (downloaded separately)
‚îÇ   ‚îî‚îÄ‚îÄ alignscore/                   # AlignScore weights
‚îÇ
‚îú‚îÄ‚îÄ scripts/                          # Global utilities
‚îÇ
‚îú‚îÄ‚îÄ Visualizations/                   # Generated plots & figures
‚îÇ
‚îî‚îÄ‚îÄ [CSV files]                       # Model inventory & tracking
```

## üöÄ Quick Start

### 1. Environment Setup

```bash
# Clone repository
git clone https://github.com/Mashrafi27/MICCAI2026-3DMedVLMS.git
cd MICCAI2026-3DMedVLMS

# Create conda environment
conda create -n vl-med-3d python=3.11 -y
conda activate vl-med-3d

# Install core dependencies
pip install -r requirements.txt

# Install model-specific dependencies (optional, per model)
cd benchmarking/inference/med3dvlm && pip install -r env/requirements.txt
```

### 2. Download Dataset

```bash
# Download CT-RATE volumes (requires access credentials)
python 3D_VLM_Spatial/dataset/download_dataset.py \
  --output-dir 3D_VLM_Spatial/dataset/data_volumes \
  --split valid_fixed
```

### 3. Generate/Filter QA Pairs

```bash
# Filter out non-spatial questions using Gemini
python 3D_VLM_Spatial/scripts/filter_qa_pairs.py \
  --qa-json 3D_VLM_Spatial/spatial_qa_output.json \
  --judgments 3D_VLM_Spatial/reports/gemini_judgments.json \
  --output 3D_VLM_Spatial/spatial_qa_filtered.json

# Convert to JSONL format for model inference
python 3D_VLM_Spatial/scripts/to_jsonl.py \
  --input 3D_VLM_Spatial/spatial_qa_filtered.json \
  --output 3D_VLM_Spatial/spatial_qa_filtered.jsonl \
  --image-root 3D_VLM_Spatial/dataset/data_volumes/dataset/valid_fixed
```

### 4. Preprocess Data for Model

Example: M3D model (resize to 256√ó256√ó32, normalize, save as .npy)

```bash
python 3D_VLM_Spatial/preprocess/preprocess_m3d.py \
  --input-jsonl 3D_VLM_Spatial/spatial_qa_filtered.jsonl \
  --nifti-root 3D_VLM_Spatial/dataset/data_volumes/dataset/valid_fixed \
  --output-root 3D_VLM_Spatial/preprocess/m3d_outputs \
  --output-jsonl 3D_VLM_Spatial/preprocess/m3d_processed.jsonl
```

### 5. Run Model Inference

Example: Med3DVLM inference

```bash
python benchmarking/inference/med3dvlm/run_custom_eval.py \
  --dataset 3D_VLM_Spatial/spatial_qa_filtered.jsonl \
  --model-path checkpoints/med3dvlm \
  --output-dir 3D_VLM_Spatial/reports \
  --batch-size 4 \
  --gpu-id 0
```

### 6. Evaluate Results

```bash
# Exact-match accuracy
python 3D_VLM_Spatial/scripts/evaluate_predictions.py \
  --predictions 3D_VLM_Spatial/reports/med3dvlm_predictions.jsonl \
  --report 3D_VLM_Spatial/reports/med3dvlm_eval.json \
  --show-mismatches

# LLM-based evaluation (clinically-aware scoring)
python 3D_VLM_Spatial/scripts/evaluate_with_gemini.py \
  --predictions 3D_VLM_Spatial/reports/med3dvlm_predictions.jsonl \
  --output 3D_VLM_Spatial/reports/med3dvlm_gemini_eval.json \
  --model models/gemini-2.0-flash

# AlignScore (factual consistency)
python 3D_VLM_Spatial/scripts/evaluate_with_alignscore.py \
  --predictions 3D_VLM_Spatial/reports/med3dvlm_predictions.jsonl \
  --output 3D_VLM_Spatial/reports/med3dvlm_alignscore.json

# Build cross-model comparison matrix
python 3D_VLM_Spatial/scripts/build_correctness_matrix.py \
  --reports-dir 3D_VLM_Spatial/reports \
  --pattern "*_predictions_*.jsonl" \
  --output 3D_VLM_Spatial/reports/correctness_matrix.csv
```

### 7. Visualize Results

```bash
# Generate answer length distributions
python 3D_VLM_Spatial/scripts/plot_answer_length_distributions.py \
  --dataset-json 3D_VLM_Spatial/spatial_qa_filtered.json \
  --predictions-dir 3D_VLM_Spatial/reports \
  --pred-glob "*_predictions_full.jsonl" \
  --output 3D_VLM_Spatial/reports/answer_length_distributions.png
```

## üìà Evaluated Models

| Model | Architecture | Input | Spatial QA Acc. | Notes |
|-------|--------------|-------|-----------------|-------|
| **Med3DVLM** | CLIP + 3D CNN | 3D volumes | - | State-of-the-art 3D medical VLM |
| **CT-Chat** | Multimodal LLM | Multi-view slices | - | Radiologist-aligned VLM |
| **M3D-LaMed** | 3D ViT + LLaMA | 256√ó256√ó32 voxels | - | 3D spatial encoding |
| **RadFM** | Foundation Model | Radiograph patches | - | Radiology-specific pretraining |
| **MedGemma-1.5** | Small LLM | Text + 2D images | - | Efficient medical LLM |
| **Merlin** | Multi-task encoder | Multi-view CT | - | Report generation focused |
| **VILA-M3 + VISTA3D** | Hybrid expert system | VISTA3D segmentation | - | 3D segmentation + 2D vision-language |
| **[Additional models]** | - | - | - | Under evaluation |

> **Note**: Accuracy scores pending final evaluation runs. Check `3D_VLM_Spatial/reports/correctness_matrix.csv` for latest results.

## üî¨ Methodology

### QA Generation & Validation Pipeline

1. **Extract from Reports**: Parse findings and impressions sections from radiologist reports
2. **Generate Candidates**: Use templated prompts + LLM to generate spatial QA candidates
3. **LLM Filtering**: Gemini 2.0 validates:
   - **Spatial relevance**: Requires anatomical reasoning (position, distance, orientation)
   - **Ground truth validity**: Answerable from imaging findings alone
   - **Non-redundancy**: Not just textual paraphrasing
4. **Final Dataset**: ~1,000 validated spatial QA pairs

### Evaluation Metrics

- **Exact-Match Accuracy**: Normalized string matching (lowercase, punctuation-agnostic)
- **LLM-Based Scoring**: Gemini judges clinical correctness accounting for medical synonyms
- **Text Metrics**: BLEU, ROUGE-L, METEOR for answer quality
- **Factual Consistency**: AlignScore measures factual alignment between prediction and image
- **Cross-Model Analysis**: Confusion matrices, per-category performance, failure mode classification

### Input Representations Tested

- **3D Voxels**: Full volumetric encoding (preferred for spatial understanding)
- **Multi-view Slices**: Axial, coronal, sagittal projections (2D-friendly models)
- **Segmentation-Augmented**: VISTA3D expert segmentation overlays (VILA-M3)
- **Slice Grids**: Combined views in canvas format (visual context)

## üõ†Ô∏è Detailed Guides

For detailed setup and running instructions per model:

- [3D_VLM_Spatial Guide](3D_VLM_Spatial/README.md) ‚Äì Dataset, filtering, evaluation
- [QA Generation v2 Guide](3D_VLM_Spatial/qa_generation_v2/README.md) ‚Äì Generate new QA pairs
- [Benchmarking Guide](benchmarking/inference/README.md) ‚Äì Model-specific inference setup
- Model-specific READMEs in `benchmarking/inference/<model>/`

## üìù Data Formats

### Input: JSONL Format

```json
{
  "image_path": "/abs/path/to/case001.nii.gz",
  "question": "Which kidney contains the larger lesion?",
  "answer": "Right kidney"
}
```

### Output: Predictions JSONL

```json
{
  "image_path": "/abs/path/to/case001.nii.gz",
  "question": "Which kidney contains the larger lesion?",
  "answer": "Right kidney",
  "prediction": "The right kidney has a larger lesion",
  "correct": false
}
```

## üîë Key Features

### Reproducibility
- ‚úÖ Complete preprocessing pipelines for each model
- ‚úÖ Frozen dataset with validation splits
- ‚úÖ Exact hyperparameters and model versions documented
- ‚úÖ W&B logging for all benchmark runs

### Extensibility
- ‚úÖ Modular model interfaces for easy addition of new models
- ‚úÖ Pluggable evaluation metrics (add custom scorers)
- ‚úÖ Flexible input format handling (JSONL, CSV, folder structures)

### Transparency
- ‚úÖ LLM judge prompts fully documented
- ‚úÖ Failure mode analysis and error categorization
- ‚úÖ Per-question performance tracking

## üìö Citations

If you use this benchmark in your work, please cite:

```bibtex
@inproceedings{MICCAI2026-3DMedVLMS,
  title={Benchmarking 3D Medical Vision-Language Models for Spatial Understanding},
  author={[Authors]},
  booktitle={Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
  year={2026}
}
```

**Key referenced works**:
- Med3DVLM: https://arxiv.org/abs/2503.20047
- DeepTumorVQA: https://arxiv.org/pdf/2505.18915
- Merlin: https://pmc.ncbi.nlm.nih.gov/articles/PMC11230513/
- VILA-M3: https://openaccess.thecvf.com/content/CVPR2025/papers/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge_CVPR_2025_paper
- RadFM: https://arxiv.org/abs/2511.18876
- AlignScore: https://arxiv.org/abs/2305.05252

## üîê Requirements & Dependencies

Primary dependencies:
- Python 3.11+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU inference)
- Google Generative AI SDK (for Gemini evaluation)
- SimpleITK (for NIfTI handling)
- tqdm, numpy, pandas

See `requirements.txt` for the full dependency list.

## üìã Acknowledgments

- **CT-RATE Dataset** creators and radiologist annotators
- **Model Teams**: Med3DVLM, CT-Chat, M3D-LaMed, RadFM, Merlin, VILA-M3 developers
- **Evaluation Tools**: AlignScore authors, NLTK contributors
- **Infrastructure**: HPC cluster support for large-scale benchmarking

## üìß Contact & Support

For questions, issues, or contributions:
- Open an issue on GitHub
- Contact: [author email]
- Lab: [institution]

## üìÑ License

This project is released under the [MIT/Apache 2.0] License. Individual model components maintain their original licenses (see respective `benchmarking/inference/<model>/` folders).

---

**Last Updated**: February 2026  
**Paper Status**: Under Review (MICCAI 2026)  
**Dataset Version**: v1.0 (Final)
